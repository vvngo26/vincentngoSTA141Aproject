---
title: "Predicting Feedback from Mice"
author: "Vincent Ngo"
date: "March 18, 2024"
output: 
  html_document:
    code_folding: hide
---
```{r echo=TRUE, eval=TRUE, include = FALSE, fig.align='center'}

# Introduction ######################
library(tidyverse)
library(knitr)
library(glmnet)
library(MASS)

session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./sessions/session',i,'.rds',sep=''))
  #print(session[[i]]$mouse_name)
}

Cori = list(session[[1]], session[[2]], session[[3]])
Forssmann = list(session[[4]], session[[5]], session[[6]], session[[7]])
Hench = list(session[[8]], session[[9]], session[[10]], session[[11]])
Lederberg = list(session[[12]], session[[13]], session[[14]], session[[15]])



# Exploratory analysis and Data integration ######################
trials = numeric(18)
neurons = numeric(18)
feedback_types = list()
stim_cond = list()

for (i in 1:18){
  neurons[i] = length(session[[i]]$brain_area)
  trials[i] = length(session[[i]]$spks)
  stim_cond = unique(session[[i]]$contrast_right)
  feedback_types = unique(session[[i]]$feedback_type)
}


brain_area = list()
for (i in 1:18){
  brain_area[[i]] = unique(session[[i]]$brain_area)
}

all_brain_area = unique(unlist(brain_area))
brain_area_matrix = matrix(0, nrow = 18, ncol = length(all_brain_area), dimnames = list(1:18, all_brain_area))
for (i in 1:18) {
  session_areas = unique(session[[i]]$brain_area)
  brain_area_matrix[i, session_areas] = 1
}

heatmap(t(brain_area_matrix), Rowv = NA, Colv = NA, col = c("white", "orange"), scale = "none", xlab = "sessions", ylab = "brain areas")

# Creating a sub-data frame
data = list()

for (i in 1:18){
data[[i]] = tibble(
    feedback = as.factor(session[[i]]$feedback_type),
    contrast_left = as.factor(session[[i]]$contrast_left),
    contrast_right = as.factor(session[[i]]$contrast_right),
    outcome = rep('name', trials[i]),
    spikes_mean = rep(0, trials[i]))

  # Determine outcomes
  for (j in 1:trials[i]){
    if (session[[i]]$contrast_left[j] > session[[i]]$contrast_right[j]){
      data[[i]]$outcome[j] = 1
    }
    else if (session[[i]]$contrast_left[j] < session[[i]]$contrast_right[j]){
      data[[i]]$outcome[j] = 2
    }
    else if (session[[i]]$contrast_right[j] == 0 & session[[i]]$contrast_right[j] == 0){
      data[[i]]$outcome[j] = 3
    }
    else{
      data[[i]]$outcome[j] = 4
    }

    # spikes
    number_spikes = session[[i]]$spks[[j]]
    #data[[i]]$spike_i_mean[j] = sum(session[[i]]$spks[[j]]) / length(session[[i]]$spks[[1]])

    total_spikes = apply(number_spikes, 1, sum)
    data[[i]]$spikes_mean[j] = mean(total_spikes)
  }
data[[i]]$outcome = as.factor(data[[i]]$outcome)
}

# Plot showing success rates in each trial
success = list()
for (i in 1:18){
  success[i] = sum(unlist(data[[i]]$feedback) == "1") / trials[i]
}
plot(1:18, success, xlab = "session", ylab = "success rate")



# Boxplot of average spikes in each session
summary_df = list()
for (i in 1:18) {
  summary_df[[i]] = summary(data[[i]]$spikes_mean)
}
summary_matrix = do.call(cbind, summary_df)
boxplot(summary_df, xlab = "session", ylab = "average spikes ")

# boxplot for each mouse
summary_Cori = list()
for (i in 1:3) {
  summary_Cori[[i]] = summary(data[[i]]$spikes_mean)
}
summary_Forssmann = list()
for (i in 4:7) {
  summary_Forssmann[[i]] = summary(data[[i]]$spikes_mean)
}
summary_Hench = list()
for (i in 8:11) {
  summary_Hench[[i]] = summary(data[[i]]$spikes_mean)
}
summary_Lederberg = list()
for (i in 12:18) {
  summary_Lederberg[[i]] = summary(data[[i]]$spikes_mean)
}
summary_Cori = discard(summary_Cori, is.null)
summary_Forssmann = discard(summary_Forssmann, is.null)
summary_Hench = discard(summary_Hench, is.null)
summary_Lederberg = discard(summary_Lederberg, is.null)

par(mfrow = c(2,2))
boxplot(summary_Cori, xlab = "session", ylab = "average spikes", main = "Cori")
boxplot(summary_Forssmann, xlab = "session", ylab = "average spikes", main = "Forssmann")
boxplot(summary_Hench, xlab = "session", ylab = "average spikes ", main = "Hench")
boxplot(summary_Lederberg, xlab = "session", ylab = "average spikes", main = "Lederberg")
par(mfrow = c(1,1))






# Predictive modeling ######################
set.seed(1)
data_merge = bind_rows(data[[1]], data[[18]])
sample = sample.int(n = length(data_merge$feedback), size = floor(.8 * length(data_merge$feedback)), replace = F)
train = data_merge[sample, ]
test  = data_merge[-sample, ]


# SESSION 7 MODEL
sample7 = sample.int(n = length(data[[7]]$feedback), size = floor(.8 * length(data[[7]]$feedback)), replace = F)
train7 = data[[7]][sample7, ]
test7  = data[[7]][-sample7, ]
# GLM
glm7 = glm(feedback~., data = train7, family="binomial")
predictions7 = predict(glm7, test, type = "response")
predictions7 = ifelse(predictions7 > 0.5, "1", "-1")
#LDA
lda7 = lda(feedback~., data = train7)
predictions7 = predict(lda7, test)$class


# SESSION 1 + 18 MODEL
set.seed(1)
glm = glm(feedback~., data = train, family="binomial")
glm_predictions = predict(glm, test, type = "response")
glm_predictions = ifelse(glm_predictions > 0.5, "1", "-1")

lda = lda(feedback~., data = train)
lda_predictions = predict(lda, test)$class





# Predictive performance ###################
glm_confusion_matrix = table(test$feedback, glm_predictions)
glm_confusion_matrix
glm_misclassification_er = 1 - sum(diag(glm_confusion_matrix)) / sum(glm_confusion_matrix)
glm_misclassification_er

lda_confusion_matrix = table(test$feedback, lda_predictions)
lda_confusion_matrix
lda_misclassification_er = 1 - sum(diag(lda_confusion_matrix)) / sum(lda_confusion_matrix)
lda_misclassification_er

par(mfrow = c(2,2))
plot(glm)
par(mfrow = c(1,1))







perform_test = list()
for(i in 1:2){
  perform_test[[i]]=readRDS(paste('./test/test',i,'.rds',sep=''))
}


data_perform = list()
for (i in 1:2){
data_perform[[i]] = tibble(
    feedback = as.factor(perform_test[[i]]$feedback_type),
    contrast_left = as.factor(perform_test[[i]]$contrast_left),
    contrast_right = as.factor(perform_test[[i]]$contrast_right),
    outcome = rep('name', 100),
    spikes_mean = rep(0, 100))

  # Determine outcomes
  for (j in 1:100){
    if (perform_test[[i]]$contrast_left[j] > perform_test[[i]]$contrast_right[j]){
      data_perform[[i]]$outcome[j] = 1
    }
    else if (perform_test[[i]]$contrast_left[j] < perform_test[[i]]$contrast_right[j]){
      data_perform[[i]]$outcome[j] = 2
    }
    else if (perform_test[[i]]$contrast_right[j] == 0 & perform_test[[i]]$contrast_right[j] == 0){
      data_perform[[i]]$outcome[j] = 3
    }
    else{
      data_perform[[i]]$outcome[j] = 4
    }

    # spikes
    number_spikes = perform_test[[i]]$spks[[j]]
    #data[[i]]$spike_i_mean[j] = sum(session[[i]]$spks[[j]]) / length(session[[i]]$spks[[1]])

    total_spikes = apply(number_spikes, 1, sum)
    data_perform[[i]]$spikes_mean[j] = mean(total_spikes)
  }
data_perform[[i]]$outcome = as.factor(data_perform[[i]]$outcome)
}

```
## Abstract
\ \ \ \ This project is an analysis of a subset of data collected by Steinmetz et al. (2019). The study comprises 39 sessions involving 10 mice, where visual stimuli were presented to the mice, and their responses were recorded along with neural activity data. The primary objective of the project is to build a predictive model to anticipate the outcome of each trial based on neural activity and stimulus information. The mice were required to make decisions based on the visual stimuli, using a wheel controlled by their fore paws.
\ \ \ \ A reward or penalty was subsequently administered based on the outcome of their decisions. When left contrast > right contrast, success (1) if turning the wheel to the right and failure (-1) otherwise. When right contrast > left contrast, success (1) if turning the wheel to the left and failure (-1) otherwise. When both left and right contrasts are zero, success (1) if holding the wheel still and failure (-1) otherwise. When left and right contrasts are equal but non-zero, left or right will be randomly chosen (50%) as the correct choice. 
  The activity of the neurons in the mice's visual cortex was recorded during the trials and made available in the form of spike trains, which are collections of timestamps corresponding to neuron firing. In this project, we focus specifically on the spike trains of neurons from the onset of the stimuli to 0.4 seconds post-onset. In addition, we only use 18 sessions (Sessions 1 to 18) from four mice: Cori, Frossman, Hence, and Lederberg.

# Introduction

\ \ \ \ A total of 18 RDS files are provided that contain the records from 18 sessions. For each session there are varying number of trials which can be accessed using `trials` which results in the following outputs: `r trials`. In addition to the number of trials, within each trial there exists the number of neurons which are observed which can be accessed using `neurons`. There are then the results of what is observed which are stimulus conditions and feedback types which are accessed in `stim_cond` and `feedback_types` respectively.

\ \ \ \ We will utilize the tidyverse, knitr, glmnet, and MASS libraries to perform our data analysis. As analyzed in the code, Cori represents the first three sessions, Forssmann represents the fourth to seventh sessions, Hench represents the eighth to eleventh sessions and Lederberg represents the twelfth to eighteenth sessions. We can then separate the sessions into each mouse to gain insight on an individual mouse if needed.

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}

```

# Exploratory analysis and Data integration

\ \ \ \ There are five variables are available for each trial, namely `feedback_type`: type of the feedback, 1 for success and -1 for failure. `contrast_left`: contrast of the left stimulus. `contrast_right`: contrast of the right stimulus. `time`: centers of the time bins for `spks`. `spks`: numbers of spikes of neurons in the visual cortex in time bins defined in `time`. brain_area`: area of the brain where each neuron lives.


```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
heatmap(t(brain_area_matrix), Rowv = NA, Colv = NA, col = c("white", "orange"), scale = "none", xlab = "sessions", ylab = "brain areas")
```

\ \ \ \ In the plot above, a scatter plot depicts which of the 62 different neurons are activated within each session. As seen in the plot, as we go up in sessions, the experimenters activate new neurons that were not present previously. We can assume that the experimenters went through the list on the y axis and activated a group of new neurons as the sessions went by.

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
boxplot(summary_df, xlab = "session", ylab = "average spikes ")

# Boxplot of average spikes in each session
par(mfrow = c(2,2))
boxplot(summary_Cori, xlab = "session", ylab = "average spikes", main = "Cori")
boxplot(summary_Forssmann, xlab = "session", ylab = "average spikes", main = "Forssmann")  
boxplot(summary_Hench, xlab = "session", ylab = "average spikes ", main = "Hench")
boxplot(summary_Lederberg, xlab = "session", ylab = "average spikes", main = "Lederberg")
par(mfrow = c(1,1))
```
\ \ \ \ Now looking at the average spikes in each session, we can see that the average number of spikes per session sits around 1.5 spikes with some outliers. In particular, session 3 and 13 are slightly higher while session 6 is quite low compared to the others. Now the range of each boxplot is around 1, so the average spikes of each session varies by about 0.5 average spikes at most. Splitting up the box plots by mice does not immediately show any correlation or noticeable patterns. Therefore more research could be done to potentially uncover some type of pattern, however that is not a concern of ours at the moment so we will skip this.

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
plot(1:18, success, xlab = "session", ylab = "success rate")
```
\ \ \ \ In this plot, we can visualize the success rate of each trial which is when the feedback type is 1. As more sessions are experimented on, the higher the success rate is. Notably, sessions 17 and 18 are at or above an 80% success rate while sessions 1-10 are almost all under 70%. This makes sense as experimenters over time would likely be able to increase the success rates using the knowledge that they have obtained from performing the previous sessions. One might choose to create their prediction model using a session where the success rate is somewhere in the middle so that the model will perform well for the lowest and highest success rates. I will test various models to see which performs best with the test data.



## Predictive modeling
```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
set.seed(1)
data_merge = bind_rows(data[[1]], data[[18]])
sample = sample.int(n = length(data_merge$feedback), size = floor(.8 * length(data_merge$feedback)), replace = F)
train = data_merge[sample, ]
test  = data_merge[-sample, ]
```

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
set.seed(1)
# SESSION 7 MODEL
sample7 = sample.int(n = length(data[[7]]$feedback), size = floor(.8 * length(data[[7]]$feedback)), replace = F)
train7 = data[[7]][sample7, ]
test7  = data[[7]][-sample7, ]
# GLM
glm7 = glm(feedback~., data = train7, family="binomial")
predictions7 = predict(glm7, test, type = "response")
predictions7 = ifelse(predictions7 > 0.5, "1", "-1")
#LDA
#lda7 = lda(feedback~., data = train7)
#predictions7 = predict(lda7, test)$class

######### RESULTS ###############
confusion_matrix7 = table(test$feedback, predictions7)
confusion_matrix7
misclassification_er7 = 1 - sum(diag(confusion_matrix7)) / sum(confusion_matrix7)
misclassification_er7
```
\ \ \ \ To build a predictive model using session 7, I input session 7 data in a logistic regression model and obtain the confusion matrix as well as the misclasssification error rate when testing using data from session 1 and 18. The result shows that the misclassification error rate is about 0.33 meaning that around 33% of the data was misclassified. I will now model using session 1 and 18 combined and compare with these results. 

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}

# SESSION 1 + 18 MODEL
set.seed(1)
glm = glm(feedback~., data = train, family="binomial")
glm_predictions = predict(glm, test, type = "response")
glm_predictions = ifelse(glm_predictions > 0.5, "1", "-1")

lda = lda(feedback~., data = train)
lda_predictions = predict(lda, test)$class


glm_confusion_matrix = table(test$feedback, glm_predictions)
glm_confusion_matrix
glm_misclassification_er = 1 - sum(diag(glm_confusion_matrix)) / sum(glm_confusion_matrix)
glm_misclassification_er

lda_confusion_matrix = table(test$feedback, lda_predictions)
lda_confusion_matrix
lda_misclassification_er = 1 - sum(diag(lda_confusion_matrix)) / sum(lda_confusion_matrix)
lda_misclassification_er



```


\ \ \ \ Constructing a predictive model using session 7 yields pretty good results. Testing using multiple seeds showed that the predictive model using session 7 data gave around a 30-40% misclassification error. This meant that around 30-40% of the data was misclassified. Therefore the success of the model was around 60-70%. However, creating a glm and lda model using both sessions 1 and 18 are producing far better results with only around a 20-30% misclassicifaction error. Thus the success of the model was around 70-80%. Using a joint session model using sessions 1 and 18 yield better results than our other model. Now the difference between the lda and glm model vary slightly meaning that our predictive modeling was very good because it accounts for multiple differences of testing.

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
par(mfrow = c(2,2))
plot(glm)
par(mfrow = c(1,1))
```
\ \ \ \ Analyzing our glm model even further reveals how the model using session 1 and 18 data out performed session 7. You can see how in some of these descriptive plots, the data seems to be split in the middle, this resulted in the lines of best fit for each of these traits to be in between the two sessions which may have given us a better model then using session 7 data. However, more evidence should be shown in order to confirm this observation.

# Predictive performance
\ \ \ \ In this section, we are given a data set in order to test our model with. There are 2 data sets both with 100 trials and randomly selected trials from session 1 and 18. We will then use previously created matrix and use the provided data in order to determine whether or not our model is successful or not using similar methods to the one we used when creating the model.
```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}

```


```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
set.seed(1)
glm = glm(feedback~., data = train, family="binomial")

perform_glm_predictions1 = predict(glm, data_perform[[1]], type = "response")
perform_glm_predictions1 = ifelse(perform_glm_predictions1 > 0.5, "1", "-1")
perform_glm_confusion_matrix1 = table(data_perform[[1]]$feedback, perform_glm_predictions1)
perform_glm_confusion_matrix1
perform_glm_misclassification_er1 = 1 - sum(diag(perform_glm_confusion_matrix1)) / sum(perform_glm_confusion_matrix1)
perform_glm_misclassification_er1
```
\ \ \ \ Here, we predict the performance of our model using the first test data. As observed, we obtain a misclasification error rate of 0.25. This means that 75% of our model was successful in correctly classifying which feedback would occur which is really similar to the results when I was creating the model.

```{r echo=TRUE, eval=TRUE, include = TRUE, fig.align='center'}
set.seed(1)
glm = glm(feedback~., data = train, family="binomial")
perform_glm_predictions2 = predict(glm, data_perform[[2]], type = "response")
perform_glm_predictions2 = ifelse(perform_glm_predictions2 > 0.5, "1", "-1")

perform_glm_confusion_matrix2 = table(data_perform[[2]]$feedback, perform_glm_predictions2)
perform_glm_confusion_matrix2
perform_glm_misclassification_er2 = 1 - sum(diag(perform_glm_confusion_matrix2)) / sum(perform_glm_confusion_matrix2)
perform_glm_misclassification_er2
```
\ \ \ \ Then we perform the same prediction but with the second test data. The misclassification error rate this time was 0.31. This means that around 69% of our model was successful in classifying feedback. This result is close to what we got when testing the first test data. This is a really promising sign that the predictive model is successful because it has shown to have worked on multiple occurrences.


# Discussion
\ \ \ \ In conclusion, modeling for feedback required exploratory data analysis in order to determine how we should make a model out of the data. Then when modeling it is important to test multiple theories to see which one works best for your own case. My model using both session 1 and 18 data yielded the best results with only around a 20-30% misclassicifaction error. Thus the success of the model was around 70-80%. There are slight variances between the glm model and lda, however, both can be used in order to predict the feedback of mice in this study. And as shown in the predictive performances, my model demonstrated very similar misclassification error rates with 2 data sets from sessions 1 and 18. Overall, I am satisfied with the results of my model as I can say that it is estimated to work around 70% of the time. There were many issues with this project such as figuring out how to access specific parts of the data set. There is still much improvement to be made. If I had more time I would have probably tried to dial down the model so that the maximum misclassification error rate would be 0.2, however, with my limited knowledge, minimal coding experience, and confusion data set, I am more than proud to have 0.25 and 0.31 error rates.





# Question of interest
The primary objective of this project is to build a predictive model to predict the outcome (i.e., feedback type) of each trial using the neural activity data (i.e., spike trains in spks), along with the stimuli (the left and right contrasts). Given the complexity of the data (and that this is a course project), we break the predictive modeling into three parts as follows.

Part 1. Exploratory data analysis. In this part, we will explore the features of the data sets in order to build our prediction model. In particular, we would like to (i) describe the data structures across sessions (e.g., number of neurons, number of trials, stimuli conditions, feedback types), (ii) explore the neural activities during each trial, (iii) explore the changes across trials, and (iv) explore homogeneity and heterogeneity across sessions and mice.

Part 2. Data integration. Using the findings in Part 1, we will propose an approach to combine data across trials by (i) extracting the shared patters across sessions and/or (ii) addressing the differences between sessions. The goal of this part is to enable the borrowing of information across sessions to enhance the prediction performance in Part 3.

Part 3. Model training and prediction. Finally, we will build a prediction model to predict the outcome (i.e., feedback types). The performance will be evaluated on two test sets of 100 trials randomly selected from Session 1 and Session 18, respectively. The test sets will be released on the day of submission when you need to evaluate the performance of your model.

# Project report outline 

The final submission of the course project is a report in HTML format, along with a link to the Github repository that can be used to reproduce your report. The project report must be legible and the exposition of the report is part of the grading rubrics. For consistency in grading, please follow the outline listed below. 

- Title.

- Abstract (5 pts).

- Section 1 Introduction (5 pts). 

- Section 2 Exploratory analysis (20 pts). 

- Section 3 Data integration (20 pts). 

- Section 4 Predictive modeling (20 pts). 

- Section 5 Prediction performance on the test sets (5 pts). 

- Section 6 Discussion (5 pts). 

In addition, the remaining 20 points will be allocated to report organization and legibility and creativity and originality. 

# Project milestones

A series of milestones are set throughout the quarter in order to encourage, and reward, early starts on the course project. Furthermore, there are several project discussion sessions throughout the quarter for students to utilize.

-Project proposal January 26th (optional): 0 points. Students are strongly recommended to attend the project discussion during the regular lecture time on Zoom.
-Milestone I February 9th (optional): 0 points but eligible for bonus points for outstanding progress or novel findings. Draft analysis and results for Part I visualization. Students are recommended to attend the optional project discussion during the regular lecture time on Zoom.
-Milestone II March 1st (optional): 0 points but eligible for bonus points for outstanding progress or novel findings. Draft analysis and results for Part II data integration. Students are recommended to attend the optional project discussion during the regular lecture time on Zoom.
-March 18th Project report: 60 points. Students are strongly recommended to attend at least one project consulting session in Week 10.
Remark: One important thing to note is that a course project is not an exam where questions on the exam are kept confidential. Instead, the instructor and TAs are more than happy to share with you our thoughts on how to improve your projects before you submit them. From a practical perspective, it is more rewarding to solicit advice and suggestions before we grade your reports than to wait for feedback afterwards. That said, we understand that you may have other courses and obligations that are more important than this course. Therefore, all submissions and attendance are optional except for the final project report due on June 12th.


# Reference {-}


Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266–273 (2019). https://doi.org/10.1038/s41586-019-1787-x


